"Auto-Complete"

N-gram models
	Speech Recognition
	Spelling Correction
	Augmentative Communication

	Create language model (LM) from text corpus to
		Estimate the probability of word sequences.

		Estimate the probability of a word following a sequence of words.

	Apply this concept to autocomplete a sentence with most likely suggestions

	MODEL
		Text Corpus --> Language Model + input ===> suggestions

	<opinion on="language models">
	Language model which we are using now is not using its brain, its just doing statistics
		See for models that use brain to do, not just rely on data

		If we just rely on data it will need too much I am guessing>>
	</opinion>

	This week you are going to learn to:
		Process a text corpus to N-gram language model

		Handle out of vocabulary words

		Implement smoothing for previously unseen N-grams

		Language model evaluation

N-grams and Probabilities
	Unigrams: set of all single words
	Bigrams: set of consecutive words of length 2

	Notation:
		w[^3][_1] = w_1 w_2 w_3

	Probability of a bigram:-
		P(y|x)
			where x is the first word

Sequence Probabilities
	P( the teacher drinks tea ) = P(the) * P( teacher the) * P( drinks the teacher) * P(teaâˆ£the teacher drinks )

	Markov Property is then applied
		because as the length of sentence gets longer, the probabilities vanish

		For Bigram:-
			We see only one behind

		For N-gram:-
			We see only N-1 words behind

		Much more practical model

		<thoughts about="markov model">
			But at what costs?

			There is definitely link between the first word of the paragraph and the last word of the paragraph.
		</thoughts>

Starting and Ending Sentences
	Bigram --> sliding window of 2

	Starting Sentences:-
		In Bigram:-
			The first one doesnt have pair
			It is fixed by using <s> as the starting word

		To generalize:-
			Add N-1 <s> at the start of the sentence

	Ending Sentences:-
		Add </s> at the end

The N-gram Language Model
	Count matrix
	Probability matrix
	Language model
	Log probability to avoid underflow
	Generative language model

	Count matrix

	Probability matrix
		Count matrix with condition that each row sums to 1

	Language Model
		Split the sentence into n-grams and find the probability of the sentence

		To Predict next:-
			Take the last (n-1)-gram from end of sequence
				then refer to the table and tell the word with highest probability

		 We are multiplying Probabilities
		 	Because they are numerically close to 0
		 	Underflow 100%
		 	So do log

		 	After doing log --> multiplication becomes addition

Language Model Evaluation
	Metric --> "Perplexity" --> evaluating Language Models

	Perplexity
		Measure of the complexity in a sample of texts

		How complex the text is

		It checks if it was written by human or it was generated by simple program choosing words at random

		Humans' text --> lower pp score
		Random word choice text --> higher pp score

		/\/\/\/\
		Probabilities of all sentences in test set and raise to (-1/m)
		\/\/\/\/

		Perplexity --> closely related to "ENTROPY"

		60 to 20 is good

		PP for character level language models < PP for word based models

		"log PP" --> between 4.3 and 5.9

Out of Vocabulary Words (OOV)
	Closed vs. Open vocabularies

	Unknown word = OOV word

	One way
		Special tag -->  <UNK>
		Minimum word frequency --> f

	Use <UNK> 's sparingly

Smoothing
	When training an N-gram on a limited corpus, the probabiltieis fo some words can be skewed.

	Problem: some N-grams missing

	1. Add-one smoothing (Laplacian Smoothing)
		For calculating probabilties of n-grams
			Add 1 in numerator
			Add V in denominator

		Add k-smoothing --> for bigger corpuses
			Add k in numerator
			Add k*V in denominator

	2. Backoff
		Lets say u r using n-grams
		If that is not present
		Use (n-1)gram and so on

		- Probability discounting e.g. Katz backoff
		- Stupid backoff

	3. Interpolation
		Linear interpolation of all <n grams
			Less than n-grams

		lambda's are found from validation set

Week Summary
	N-grams and probabilties
	Approximate sentence probability from N-grams
	Build language model from corpus
	Fix missing information
	Envaluate language model with perplexity

