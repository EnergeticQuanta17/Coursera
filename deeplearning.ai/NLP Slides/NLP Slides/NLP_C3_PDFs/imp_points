<---------------------Week 1-------------------------------->
NEURAL NETWORKS FOR SENTIMENT ANALYSIS


Feature extraction
Supervised machine learning
Binary classification
Text preprocessing
ReLU
Python classes
Trax
Neural networks

Neural Networks for Sentiment Analysis
Language Generator usign RNNs
LSTM --> Named Entity Recognition
Siamese Networks

Trax: Neural Networks
	Built on TensorFlow

	Advantages of using frameworks
		Run fast on CPUs, GPUs and TPUs
		Parallel Computing
		Record of algebraic computation for gradient evaluation

		TensorFlow
		PyTorch
		JAX

	from trax import layers as tl
	Model = tl.Serial(
			tl.Dense(4),
			tl.Sigmoid(),
			tl.Dense(4),
			tl.Sigmoid(),
			tl.Dense(3),
			tl.Softmax(),
				
		)


Trax best for
	Machine Learning Research best
	Especially Sequence Models and Transformeres and Models used in NLP

	Tensor2Tensor library

	JAX: fastest Transformer in MLPerf 2020

Have downloaded a nice tutorial notebook on trax

trax
	Layers
		Relu Layer
		Concatenate Layer
		Layers are Configurable
		Layers can have Weights
		Layers can have Weights
		LayerNorm
		Custom Layers

	Combinators
		Serial Combinator
		Parallel Combinator

	JAX
		Trax's JAX compatible numpy

Other layers could include embedding layers and mean layers.
	The mean layer allows you to take the average of the embeddings. 

	This layer does not have any trainable parameters. 

Finding gradient in trax
	def f(x):
		return x**2
	trax.math.grad(f)

	y = model(x)
	grads = grad(y.forward)(y.weights, x)

		weights -= alpha*grads



<---------------------Week 2-------------------------------->

Recurrent Neural Networks

BiDirectional RNNs
	Can keep track of information from both directions

	and Allows better character generation

	More layers --> more abstract dependencies

Gated RNNs
	More powerful than vanilla RNNs

Traditional Language models
	Large N-grams to capture dependencies between distant words

	Need a lot of space and RAM

	Solution: RNNS

Advantages of Recurrent Neural Networks
	W_h --> for propogation
	W_x --> for recurrent part

	Only learnable parameters are W_x and W_h

	They propogate information between sequences

	Lots of computations share parameters

Applications of RNNs
	Machine Translation and Caption Generation

	One to One
	One to Many - Image to text
	Many to One - Sentiment Analysis
	Many to Many - Machine Translation

Many to Many
	Encoder-Decoder

One to One: given some scores of a championship, you can predict the winner. 

One to Many: given an image, you can predict what the caption is going to be.

Many to One: given a tweet, you can predict the sentiment of that tweet. 

Many to Many: given an english sentence, you can translate it to its German equivalent. 

Math in Simple RNNs
	Basic recurrent units have two inputs at each time
		h<t-1> and x<t>

	Hidden states propogate information through time

Cost Function for RNNs
	Cross-Entropy to give for all times

	Individual loss: Average with respect to time

Implementation Note
	scan() functions

	Takes a function and apply it to all elements from the beginning to the end

	Need this type of abstractions to parallelize the code

	tf.scan() mimics RNNs

	Note, that is basically what an RNN is doing. It takes the initializer, and returns a list of outputs (ys), and uses the current value, to get the next y and the next current value. These type of abstractions allow for much faster computation. 

Gated Recurrent Units
	Allows relevant information to be kept in the hidden state even for long sequences

	"Relevance and update" gates to remember important prior information

	Both Relevant gates and Update gates are sigmoid outputs

	Hidden state candidate

	GRUs decide how to update the hidden state

	GRUs help preserve important information



Vanilla RNNs vs. Gated Recurrent Units

Deep and Bi-directional RNNs
	Deep captures dependencies which cannot be captured by shallow RNnS

	Stackign RNNs --> deep RNNs

	Bi-directional RNNs
		In bidirectional RNNs, it is not necessary to first perform forward propagation from left to right  in order to do it in the opposite direction.

	1. Get hidden states for current layer
	2. Pass the activations to the next layer

	Bi-directional RNNs are important, because knowing what is next in the sentence could give you more context about the sentence itself. 



<---------------------Week 3-------------------------------->

LSTMs and Named Entity Recognition

Vanishing and Exploding gradients problems faced by conventional RNNs

Contribution of the hidden state to the gradients approaches zero as we move further away from the place where the loss is computed
	Conversely the when the gradients are much bigger than 1 --> it can go to infinity

	less 1 --> Vanishing gradients


Solutions for Vanishing and Exploding gradients problems
	Identity RNN with ReLU activation
		Initialize weights to identity matrix

		Only works for Vanishing gradients

	Gradient Clipping
		Any value greater than 25 is put to 25

	Skip connections

Advantages of RNNs
RNNs allow us to capture dependancies within a short range and they take up less RAM than other n-gram models. 

Disadvantages of RNNs
RNNs struggle with longer term dependencies and are very prone to vanishing or exploding gradients.

https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/

LSTM
	Best known solutions to vanishing gradients problem

	Learns what to remember and when to forget

	Basic anatomy
		A cell state
		A hidden state
		Multiple gates

	Typical LSTMs have a cell and three gates:-
		Forget gate
		Input gate
		Output gate

	Input gate: tells you how much information to input at any time point. 

	Forget gate: tells you how much information to forget at any time point. 

	Output gate: tells you how much information to pass over at any time point. 

Applications of LSTMs
	Next-character prediction
	Chatbots
	Music composition
	Image captioning
	Speech Recognition

LSTM Architecture
	See pic

	https://www.coursera.org/learn/sequence-models-in-nlp/supplement/RHssB/lstm-architecture

Named Entity Recognition
	Named Entity Recognition (NER) locates and extracts predefined entities from text. It allows you to find places, organizations, names, time and dates.

	Geographical
	Organization
	Geopolitical
	Time Indicator
	Artifacts
	Person

Applications of NER systems
	Search Engine Efficiency
	Recommendation engines
	Customer service
	Automatic trading

LSTM Equations
	https://www.coursera.org/learn/sequence-models-in-nlp/supplement/ANbcf/lstm-equations-optional

Training NERs: Data Processing
	Assign each entity class a unique number

	Assign each word the number

	Token padding
		As LSTMs all the sequences need to be the same size

		<PAD> token to fill empty spaces

	Training the NER
		1. Create a tensor for each input and its corresponding number
		2. Put them in a batch of power of 2
		3. Feed it into an LSTM unit
		4. Run the output through a dense layer
		5. Predict using a log softmax over K classes

		Log softmax has better numerical performance

	Layers in TRAX
		model = tl.Serial(
				tl.Embedding(),
				tl.LSTM(),
				tl.Dense(),
				tl.LogSoftmax()
			)

	Note that this is just one example of an NER system. You can have different architectures. 

Computing Accuracy
	Evaluating the model
		1. Pass test set through the model
		2. Get arg max across the prediction array
		3. Mask padded tokens
		4. Compare outputs against test labels

	If padding tokens, remember to mask them when computing accuracy



<-------------------Week 4 ------------------------------->

SIAMESE NETWORKS

Triplet Loss

A Siamese Network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors.
	We can then compare the output and see if they are similar

Siamese networks, a special type of neural network made of two identical networks that are eventually merged together, then build your own Siamese network that identifies question duplicates in a dataset from Quora.

Neural network made of two "identifiesical" neural networks which are merged at the end.

Identify similarity between things

Applications of Siamese Networks
	Handwritten checks
	Question duplicates
	Queries

Architecture
	Merged through a disc layer to produce a final output or a similarity score

	embedding --> LSTM --> v1
							  \
							   \
							    -----> cosine similarity --> y_hat
							   /
							  /
	embedding --> LSTM --> v2 

	y_hat greater than threshold --> the two questions are same
		else different

	These two sub-networks are sister-networks which come together to produce a similarity score. Not all Siamese networks will be designed to contain LSTMs. One thing to remember is that sub-networks share identical parameters. This means that you only need to train one set of weights and not two. 

	The output of each sub-network is a vector. You can then run the output through a cosine similarity function to get the similarity score. In the next video, we will talk about the cost function for such a network.

Cost Function
	Triplet Loss

	Anchor, (positve or negative questions)

	s(A,P) = 1
	s(A,N) = -1

Triplets
	diff = s(A,N) - s(A,P)

	With non-linearity
		L = 0 for diff <=0
			diff for diff>0

	With alpha margin
		L = 0 if (diff + alpha) <= 0
			(diff + alpha) if (diff + alpha) > 0

	L(A,P,N) = max(diff + alpha, 0)

	Triplets Selection
		Random triplets --> easy to satisfy
			Little is there to learn

		Hard triplets
			Harder to train
			More to learn

			Model gets most information

	ùú∂: controls how far cos(A,P) is from cos(A,N)

	Easy negative triplet: cos(A,N) < cos(A,P)

	Semi-hard negative triplet:  cos(A,N) < cos(A,P) < cos(A,N) + ùú∂ 

	Hard negative triplet: cos(A,P) < cos(A,N)

Computing The Cost I
	Prepare the batches

	Matrix reprsenting the similarity between the two groups

	Hard Negative Mining

	Each horizontal vector corresponds to the encoding of the corresponding question. Now when you multiply the two matrices and compute the cosine, you get the following: 


	The diagonal line corresponds to scores of similar sentences, (normally they should be positive). The off-diagonals correspond to cosine scores between the anchor and the negative examples. 

Computing The Cost II
	Mean negative
		Mean of off-diagonal in each row

	Closest negative
		OFf-diagonal values closest to the value on diagonal in each row

	Hard Negative Mining
		L_original = max(s(A,N) - s(A,P) + alpha, 0)

		L_1 = max(mean_negative - s(A,P) + alpha, 0)
			Reduces the noise, by taking mean
			Because average of the noise is assumed to be 0

		L_2 = max(closest_negative - s(A,P) + alpha, 0)

		L_full(A,P,N) = L_1 + L_2

		J = summation of all L_full

One-Shot Learning
	Measure similarity between 2 classes

	Makes use of Siamese Networks

Training / Testing

