<---------------Week 1----------------->
Neural Machine Translation

Discover some of the shortcomings of a traditional seq2seq model and how to solve for them by adding an attention mechanism, then build a Neural Machine Translation model with Attention that translates English sentences into German.

Explain how an Encoder/Decoder model works
Apply word alignment for machine translation
Train a Neural Machine Translation model with Attention
Develop intuition for how teacher forcing helps a translation model check its predictions
Use BLEU score and ROUGE score to evaluate machine-generated text quality
Describe several decoding methods including MBR and Beam search

Vanishing gradients for very long sentences

Attention mechanism --> to access all relevant parts of the input sentence regardless of its length

LSTM with attention

Greedy decoding, random something and beam search

Seq2seq (2014)
	LSTM for encoding and decoding

	Variable length sequences to fixed length memory
		which encodes the overall meaning of sentences

	Inputs and outputs can have different lengths

	LSTMs and GRUs to avoid vanishing and exploding gradient problems

	Encoder and a Decoder
		Encoder --> output is "hidden" --> input to Decoder --> output is "translated sentence"

	Seq2seq Decoder
		Input to this is hidden state and embedding of <SOS>
			start of sequence token

		and then output of the previous becoems input of current

	The information bottleneck
		Since we use fixed length memory for the hidden states

		Long sequences become problematic

		Only a fixed amount of information can be passed from encoder to decoder

		"attention" is the solution

Attention layer instead of "hidden"
	The model can focus on specific hidden states at every step

Seq2seq Model with Attention
	Allows to focus where the model is looking at whenever making a prediction

	Uses all hidden state outputs

	Pointwise addition to combine the information --> to get the "context vector"

	This is passed to the Decoder instead

	Weighing the improtant words more than the others

	Compare the decoder states with each encoder state to determine the most important inputs

	Attention

Queries, Keys, Values, and Attention
	First attention paper in 2014

	Query
		Query is associated with a Key and its value is returned

		Query and Key are in different languages

	Similarity between the words (of query and key) is called alignment
		Similarity is used in for weighted sum

	Using Query and key we can calculate alignment scores that are measures of how well the query and keys match

	These alignment scores are then turned into weights used for weighted sum of the value vectors

	For each key in the attention mechanism there is an associated weight

	And the weights of the key of attention mechanism is differnent for each query

	Weighted Sum --> Alignment Vector

	Process can be performed using "Scaled dot-product Attention"
		Using matrix representation we can calculate for multiple queries at the same time

		softmax(Q * K.T / sqrt(d_lk)) * V

			Q * K.T --> Similarity between queries and keys

			sqrt(k_t) --> scaled --> improves model performance

			softmax(*) --> so that weights sum to 1

			values are then multiplied to get the Attention Vector

		Scaled dot-product attention
			requires only 2 matrix multiplications

			which is highly optimized

			But the thing is --> alignments must be learned elsewhere

	Typically, alignment is learned in the input embeddings or in other linear layers before the attention layer

	Alignment weights
		just like we saw in previous course
		Diagonal's x and y correspomnds to similar to word translations

		Flexible attention
			Works for languages with different grammar structures

		The matrix also shows how the translation has aligned the two lanauges in the form of matrix
			where high alignment is shown with white and low is shown with black

	Attention allows a seq2seq decoder to use information from each encoder step instead of just the final encoder hidden state. In the attention operation, the encoder outputs are weighted based on the decoder hidden state, then combined into one context vector. This vector is then used as input to the decoder to predict the next output step.

Setup for Machine Translation
	Use pre-trained vector embeddings
		Otherwise, initially represent words with one-hot vectors

	Keep track of index mappings with word2ind and ind2word mappings

	Add end of sequence tokens: <EOS>

	Pad the token vectors with zeros

Teacher Forcing
	Training for NMT

	Skipping words which have wrong translations ig

	Teacher forcing  uses the actual output from the training dataset at time step y(t) as input in the next time step (t+1), instead of the output generated by your model.

NMT Model with Attention
	Encoder --> Attention Mechanism --> Decoder

	Attention Mechanism --> helps the decoder to focus on important things

	Input sequence is duplicated
	
	Input and Target is fed to
		Encoder
			and
		Pre-attention Decoder

		respectively

	Mask it and make it ready for "Attention Layer"
		While making it ready, even the copy of the input tokens is used

	Then the Attention Layer, outputs "Context Vectors" and the mask

	Discard mask

	Then pass the Context Vectors through the Decoder to get the target

BLEU Score
	BiLingual Evalution Understudy

	Compare candidate translations to reference (human) translations

	Closer to 1 --> better
	Closer to 0 --> worse

	BLEU (Modified)

	BLEU score is great but
		BLEU doesn't consider semantic meaning

		BLEU doesn't consider sentence structure

	BLEUi is precision oriented

ROUGE-N Score
	Quality of Machine Translation Systems

	Recall-Oriented Understudy of Gisting Evaluation

	Recall-Oriented by default

	ROUGE cares about how much of the human created references appear in the candidate translation

	Multiple versions of this metric

F1 Score
	Precision --> BLEU
	Recall --> ROUGE-N

	Find F1

Sampling and Decoding
	Random Sampling
	Temperature in sampling
	Greedy decoding

	Seq2seq -> Produces probability distributions

	Greedy Decoding
		Highest probability word is chosen

	Random Sampling
		Samples according to the probabilities assigned

		Problem:-		Too random
		Solution:- 		Assign more weight to more probable words, and less weight to less probable words

	Temperature
		Can control for more or less randomness in predictions

		Safe decisions --> lower temperature
			More confident
			Conservative network

		Higher temperature setting
			More excited
			Random Network


Beam search
	Technique that allows to find the best sequences over a fixed window size known as the beam width

	The problem is that initially if u choose greedily then the upcoming probabilties might be very low comapred to other paths
		Just like reinforcement learning

	Beam search decoding
		Probabiltiies of multiple possible sequences at each step

	At each step, we keep the B most probable sequences, and drop the rest

	Can be computationally expensive depending on B

	Beam search decoding
		Use the model B times to get the B most probable outputs

	Penalizes the choice of long word sequences, since it is product of multiple conditional probabilties
		Normalize to avoid this problem

	computationally expensive and consumes a lot of memory

Minimum Bayes Risk (MBR)
	To Evalute NMT systems

	Generate several candidate translations

	Assign a similarity to every pair using a similarity score (such as 'ROUGE')

	Select the sample with the highest average similarity

	**
		Find the candidate translation that maximizes
			Compare with every other candidate
				ROUGE score between the pair of candidates
	**

	Summary
		Compare several candidate translations

		Choose candidate with highest average similarity

		Better performance than random sampling and greedy decoding




<---------------Week 2----------------->
			 TRANSFORMERS

Compare RNNs and other sequential models to the more modern Transformer architecture, then create a tool that generates text summaries.

Describe the three basic types of attention
Name the two types of layers in a Transformer
Define three main matrices in attention
Interpret the math behind scaled dot product attention, causal attention, and multi-head attention
Use articles and their summaries to create input features for training a text summarizer
Build a Transformer decoder model (GPT-2)

Dot-Product attention
Causal Attention
Encoder-Decoder Attention
Self-Attention

Transformers vs RNNs
	Seq2seq Architectures
		Loss of information
		Vanishing gradient problem

	Transformers don't use RNNs
		In contrast, transformers are based on attention and don't require any sequential computation per layer, only a single step is needed. Additionally, the gradient steps that need to be taken from the last output to the first input in a transformer is just one. For RNNs, the number of steps increases with longer sequences. Finally, transformers don't suffer from vanishing gradients problems that are related to the length of the sequences. Here is an image that might help you visualize it. 

	Transformer model uses Scaled Dot-Product Attention

	The Encoder
		Multi-Head Attention
		Add & Norm

		Feed Forward
		Add & Norm

	The Decoder
		Masked Multi-Head Attention
		Add & Norm

		Multi-Head Attention
		Add & Norm

		Feed Forward
		Add & Norm

Transformers Applications
	Text Summarization
	Auto-Complete
	Named Entity Recognition
	Question Answering
	Translation
	Chat-bots

	Sentiment Analysis
	Market Intelligence
	Text Classification
	Character Recognition
	Spell Checking

SOTA Transformers
	GPT-2
	BERT
	T5

T5
	A single T5 model can do multiple differnet tasks

	T5 is a powerful multi-task Transformer



Masked Self Attention
	Self-Attention
		gives a representation of the meaning of each word within the sentence

	In Masked Self Attention
		Queries, keys and values come from the same sentence.
		Queries don't attend to future positions.

	Works like encoder decoder

	Inside softmax we add the self-attention-matrix with above diagonal elements as -inf

	Weights assigned to future positions are equal to 0

Multi-head Attention
	We apply in parallel, the attention mechanism to multiple sets of matrices that we can get by transforming the original embeddings

	In multi-head attention, the number of times we apply attention mechanism is the the number of heads in the model

	Apply scaled dot-product attention to each of the heads

	Heads are like layers in CNN

	d_k = d_v = d_model / h

	d_model --> context vector representaion

	Similar computational cost to single head attention
		Due to doing parallelly

Transformer Decoder

Transformer Summarizer


