Low Res -> High Res

GAN to synthesize more data

Two Neural Networks
	Art Forger
	Art Inspector

Week 1: Intro to GANs

Week 2: Deep Convolutional GANs

Week 3: Wasserstein GANs with Gradient Penalty

Week 4: Conditional GAN & Controllable Generation

<---------- Week 1 ---------->

Unsupervised

Generator
Discriminator

Binary cross-entropy loss

Generative Models
	Generative Models vs. Discriminative Models
		Discriminative Models 
			Features -> Class
			P(Y|X)

		Generative Models
			Noise, Class --> Features

			Noise ensures that the same thing is not generated again

			P(X|Y)

	Generative Models
		Variational Autoencoders (VAE)
			Encoder -> Latent Space -> Decoder
			Variational => Twist is that it generates a distribution over the latent space and then samples from it and sends to the Decoder

		Generative Adverserial Networks
			Generator -> Discriminator

			Generator
				Random Noise Input and Class
				Like Decoder

			Discriminator
				Sees fake and real Images

			Later the Discriminator is switched off

2014

GANs can be used for Image Translation
	CycleGAN	
		From one domain to another
			Example:- Horse running to Zebra running

	GuaGAN	
		Doodles --> Pictures

		Input is just background with different colors without details

		And the GAN produces the details for according to the color pallette

	GANs are Magic
		Potrait of Mona Lisa --> Animating face motion

	GANs for 3D objects
		They can generate 3D also
		Generative Design

	Companies using GANs
		Adobe -> next-gen photoshop
		Google -> Text Generation
		IBM -> Data Augmentation
		Snapchat/TikTok -> Image Filters
		Disney -> Super resolution

Intuition Behind GANs
	The Generator's goal is to fool the Discriminator

	The Discriminator's goal is to distinguish between real and fake

	They learn from the competition with each other

	At the end, fakes look like they are real

Discriminator
	Classifier
		Not only image classificaiton,
			but also text to "cat", 
			video to "cat",
			so on

		Modelled using Neural Network
			Output is the n neurons (for each class)
				represnting the probability of the input being that class


	Output
		0 to 1
		How much percent it believes that the image is fake


		P(fake | X)
			fake sampled from "class"
			X sampled from "features"

Generator
	Generates examples of the class

	Takes input -> Noise vector

	Output is image
		3 million pixels

	Different output at every run

	P(X|Y)	
		X is sampled from the set of "Features"
		Y ~ set of "Classes"

Binary Cross-Entropy Cost Function
	J(theta) = (-1/m) * [y * log(h(x, theta)) + (1-y) * log(1-h(x, theta))]

Putting it all together
	Discriminator --> learns from getting feedback on if its classification was correct

	Generator --> learns over time using feedback from the Discriminator

	While one is happening, other is kept constant

	Both models should improve together,
		Equal skill level

		If the skill level difference is very high then the other will find it very hard to improve.

	Discriminator's task is much easier
		Super Discriminator --> No way to improve the Generator

PyTorch
	from torch import nn

	class LogisticRegression(nn.Module):
		def __init__(self, in):
			self.log_reg = nn.Sequential(
							   nn.Linear(in, 1),
							   nn.Sigmoid()
						   )

		def forward(self, x):
			return self.log_reg(x)


	model = LogisticRegression(16)
	criterion = nn.BCELoss()

	optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

	for t in range(n_epochs):
		y_pred = model(x)
		loss = criterion(y_pred, y)

		optimizer.zero_grad()
		loss.backward()
		optimizer.step()

PyTorch
	t = torch.Tensor()

	t.device

	t.shape or t.size

	Number of elements
		t.numel()

	t[1, 1, 0]

	t[1, 1, 0].item()

	t[:, 0, 0]

	torch.ones_like(t)

	torch.zeros_like(t)

	torch.randn_like(t)

	torch.randn(2, 2, device='cpu')

	t.mean()

	t.std()

	t.mean(0)
	t.mean(dim=0)
	t.mean(axis=0)

	Similarly 
		torch.mean(t, 0)
		torch.mean(t, dim=0)
		torch.mean(t, axis=0)


	import torch.nn as nn

	linear = nn.Linear(10, 2)
	example_input = torch.randn(3, 10)
	example_output = linear(example_input)

	relu = nn.ReLU()
	relu_output = relu(example_output)

	batchnorm = nn.BatchNorm1d(2)
	batchnorm_output = batchnorm(relu_output)

	adam_opt = optim.Adam(mlp_layer.parameters(), lr=1e-1)

	requires_grad_()

	with torch.no_grad():

	detach()

	list(example_model.parameters())

	list(example_model.named_parameters())

	nn.Conv2d
	nn.COnvTranspose2d
	nn.BatchNorm2d
	nn.Upsample
	nn.functional.interpolate

	In PyTorch, we can easily transfer data from GPU to CPU
		Just apply the function
			.cpu()
		on the tensor LOL

	criterion = nn.BCEWithLogitsLoss()

Itâ€™s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It's important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments.

	
	This is how it looks to update the gradients of "Generator"	
		gen_opt.zero_grad()
        gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)
        gen_loss.backward()
        gen_opt.step()


https://www.researchgate.net/figure/Latent-space-visualization-of-the-10-MNIST-digits-in-2-dimensions-of-both-N-VAE-left_fig2_324182043

https://arxiv.org/abs/1912.04958

https://arxiv.org/abs/1903.07291

https://arxiv.org/abs/1905.08233

https://arxiv.org/abs/1809.11096



<---------- Week 2 ---------->
Deep Convolutional GANs
	Compose a Deep Convolutional GAN using these components.
	Upsampling and Transposed convolutions.

	Image Generation

Activations (Basic Properties)
	g (the Activation Function) must be
		Differentiable - for backpropagation
		Non-Linear Function - Else wont be complex

	Commonly used Activation Functions
		ReLU
			POPULAR
			max(0, z)
			Removes all the negatives
			By covention:- diff at 0 is 0

			"Dying ReLU" problem

		Leaky ReLU
			Solves the "Dying ReLU" problem
			a=0.1
			a is the leak
			a is a hyperparameter

		Sigmoid
			0 and 1

			"Vanishing gradients" problem

			"Saturating gradients" problem

			Often not used in hidden layers as the derivatives of functions approaches 0 "at the tails of the function"

		Tanh
			-1 and 1

			Negative inputs have Negative outputs

			"Vanishing gradients" problem

			"Saturating gradients" problem

	Too much time to train

Batch Normalization
	To speed up

	If mean and variance of the input variables are different
		Plotting the distributions,
			it might be broad or narrow as in high variance or low variance

			and mean at different positions

		This leads to the cost function being an ellipse

		"Covariance Shift"

	If the mean=0 and std=1, 
		Cost function --> Circular instead of ellipse

		Effect of Covariance shift is reduced significantly.

	Internal Covariance Shift
		There is Covariance Shift in the hidden layers of the NN.

		Batch Norm saves the day
			Smooths the cost function
			Reduces Internal Covariance Shift
			Speeds up learning

		Caveat:-
			Theory behind Internal Covariance Shift is not actually conlusive.

			We are still finding other reasons for which Batch Normalization is useful.

	Procedure
		Subtract mean of all the neuron values in that leyer FOR "z"
			Not for "a"

		And then divide by sqrt(sigma**2 + epsilon)
			epsilon is used for non-zero denominator

		We rescale the obtained z_cap
			Using the linear transformation --> y = gamma * z + beta

		So the thing is, after normalization u can scale however it needs
			But inputs are forced to be normalized with no scaling

		This y is then sent to Activation Function g(y)

		For Test:=
			We take mean of whole thing

		Batch Norm includes learnable shift and scale factors

		During test, the running statistics from training are used

Convolutions
	Padding and Stride

		Padding provides similar importance to the edges and the center

	Pooling and Upsampling
		Max Pooling --> getting the most salient information.

		No learnable parameters

		Upsampling
			Increases the size of the input

			Nearest Neighbours Upsampling
				Same pixels for all in the kernel

			Linear Interpolation

			Bi-Linear Interpolation

	Transposed Convolutions
		Input * Filter -> Output
		 2x2      2x2       3x3

		Checkerboard problem

		They have learnable parameters
			Checkerboard pattern problem

	Upsampling infers pixels using a predefined method, while transposed convolution learns a filter.

	https://distill.pub/2016/deconv-checkerboard/

	