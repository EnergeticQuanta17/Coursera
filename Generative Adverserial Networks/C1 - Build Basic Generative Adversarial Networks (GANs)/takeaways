Low Res -> High Res

GAN to synthesize more data

Two Neural Networks
	Art Forger
	Art Inspector

Week 1: Intro to GANs

Week 2: Deep Convolutional GANs

Week 3: Wasserstein GANs with Gradient Penalty

Week 4: Conditional GAN & Controllable Generation

<---------- Week 1 ---------->

Unsupervised

Generator
Discriminator

Binary cross-entropy loss

Generative Models
	Generative Models vs. Discriminative Models
		Discriminative Models 
			Features -> Class
			P(Y|X)

		Generative Models
			Noise, Class --> Features

			Noise ensures that the same thing is not generated again

			P(X|Y)

	Generative Models
		Variational Autoencoders (VAE)
			Encoder -> Latent Space -> Decoder
			Variational => Twist is that it generates a distribution over the latent space and then samples from it and sends to the Decoder

		Generative Adverserial Networks
			Generator -> Discriminator

			Generator
				Random Noise Input and Class
				Like Decoder

			Discriminator
				Sees fake and real Images

			Later the Discriminator is switched off

2014

GANs can be used for Image Translation
	CycleGAN	
		From one domain to another
			Example:- Horse running to Zebra running

	GuaGAN	
		Doodles --> Pictures

		Input is just background with different colors without details

		And the GAN produces the details for according to the color pallette

	GANs are Magic
		Potrait of Mona Lisa --> Animating face motion

	GANs for 3D objects
		They can generate 3D also
		Generative Design

	Companies using GANs
		Adobe -> next-gen photoshop
		Google -> Text Generation
		IBM -> Data Augmentation
		Snapchat/TikTok -> Image Filters
		Disney -> Super resolution

Intuition Behind GANs
	The Generator's goal is to fool the Discriminator

	The Discriminator's goal is to distinguish between real and fake

	They learn from the competition with each other

	At the end, fakes look like they are real

Discriminator
	Classifier
		Not only image classificaiton,
			but also text to "cat", 
			video to "cat",
			so on

		Modelled using Neural Network
			Output is the n neurons (for each class)
				represnting the probability of the input being that class


	Output
		0 to 1
		How much percent it believes that the image is fake


		P(fake | X)
			fake sampled from "class"
			X sampled from "features"

Generator
	Generates examples of the class

	Takes input -> Noise vector

	Output is image
		3 million pixels

	Different output at every run

	P(X|Y)	
		X is sampled from the set of "Features"
		Y ~ set of "Classes"

Binary Cross-Entropy Cost Function
	J(theta) = (-1/m) * [y * log(h(x, theta)) + (1-y) * log(1-h(x, theta))]

Putting it all together
	Discriminator --> learns from getting feedback on if its classification was correct

	Generator --> learns over time using feedback from the Discriminator

	While one is happening, other is kept constant

	Both models should improve together,
		Equal skill level

		If the skill level difference is very high then the other will find it very hard to improve.

	Discriminator's task is much easier
		Super Discriminator --> No way to improve the Generator

PyTorch
	from torch import nn

	class LogisticRegression(nn.Module):
		def __init__(self, in):
			self.log_reg = nn.Sequential(
							   nn.Linear(in, 1),
							   nn.Sigmoid()
						   )

		def forward(self, x):
			return self.log_reg(x)


	model = LogisticRegression(16)
	criterion = nn.BCELoss()

	optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

	for t in range(n_epochs):
		y_pred = model(x)
		loss = criterion(y_pred, y)

		optimizer.zero_grad()
		loss.backward()
		optimizer.step()

PyTorch
	t = torch.Tensor()

	t.device

	t.shape or t.size

	Number of elements
		t.numel()

	t[1, 1, 0]

	t[1, 1, 0].item()

	t[:, 0, 0]

	torch.ones_like(t)

	torch.zeros_like(t)

	torch.randn_like(t)

	torch.randn(2, 2, device='cpu')

	t.mean()

	t.std()

	t.mean(0)
	t.mean(dim=0)
	t.mean(axis=0)

	Similarly 
		torch.mean(t, 0)
		torch.mean(t, dim=0)
		torch.mean(t, axis=0)


	import torch.nn as nn

	linear = nn.Linear(10, 2)
	example_input = torch.randn(3, 10)
	example_output = linear(example_input)

	relu = nn.ReLU()
	relu_output = relu(example_output)

	batchnorm = nn.BatchNorm1d(2)
	batchnorm_output = batchnorm(relu_output)

	adam_opt = optim.Adam(mlp_layer.parameters(), lr=1e-1)

	requires_grad_()

	with torch.no_grad():

	detach()

	list(example_model.parameters())

	list(example_model.named_parameters())

	nn.Conv2d
	nn.COnvTranspose2d
	nn.BatchNorm2d
	nn.Upsample
	nn.functional.interpolate

	In PyTorch, we can easily transfer data from GPU to CPU
		Just apply the function
			.cpu()
		on the tensor LOL

	criterion = nn.BCEWithLogitsLoss()

Itâ€™s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It's important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments.

	
	This is how it looks to update the gradients of "Generator"	
		gen_opt.zero_grad()
        gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)
        gen_loss.backward()
        gen_opt.step()


https://www.researchgate.net/figure/Latent-space-visualization-of-the-10-MNIST-digits-in-2-dimensions-of-both-N-VAE-left_fig2_324182043

https://arxiv.org/abs/1912.04958

https://arxiv.org/abs/1903.07291

https://arxiv.org/abs/1905.08233

https://arxiv.org/abs/1809.11096

<---------- Week 2 ---------->
Deep Convolutional GANs
	Compose a Deep Convolutional GAN using these components.
	Upsampling and Transposed convolutions.

	Image Generation

Activations (Basic Properties)
	g (the Activation Function) must be
		Differentiable - for backpropagation
		Non-Linear Function - Else wont be complex

	Commonly used Activation Functions
		ReLU
			POPULAR
			max(0, z)
			Removes all the negatives
			By covention:- diff at 0 is 0

			"Dying ReLU" problem

		Leaky ReLU
			Solves the "Dying ReLU" problem
			a=0.1
			a is the leak
			a is a hyperparameter

		Sigmoid
			0 and 1

			"Vanishing gradients" problem

			"Saturating gradients" problem

			Often not used in hidden layers as the derivatives of functions approaches 0 "at the tails of the function"

		Tanh
			-1 and 1

			Negative inputs have Negative outputs

			"Vanishing gradients" problem

			"Saturating gradients" problem

	Too much time to train

Batch Normalization
	To speed up

	If mean and variance of the input variables are different
		Plotting the distributions,
			it might be broad or narrow as in high variance or low variance

			and mean at different positions

		This leads to the cost function being an ellipse

		"Covariance Shift"

	If the mean=0 and std=1, 
		Cost function --> Circular instead of ellipse

		Effect of Covariance shift is reduced significantly.

	Internal Covariance Shift
		There is Covariance Shift in the hidden layers of the NN.

		Batch Norm saves the day
			Smooths the cost function
			Reduces Internal Covariance Shift
			Speeds up learning

		Caveat:-
			Theory behind Internal Covariance Shift is not actually conlusive.

			We are still finding other reasons for which Batch Normalization is useful.

	Procedure
		Subtract mean of all the neuron values in that leyer FOR "z"
			Not for "a"

		And then divide by sqrt(sigma**2 + epsilon)
			epsilon is used for non-zero denominator

		We rescale the obtained z_cap
			Using the linear transformation --> y = gamma * z + beta

		So the thing is, after normalization u can scale however it needs
			But inputs are forced to be normalized with no scaling

		This y is then sent to Activation Function g(y)

		For Test:=
			We take mean of whole thing

		Batch Norm includes learnable shift and scale factors

		During test, the running statistics from training are used

Convolutions
	Padding and Stride

		Padding provides similar importance to the edges and the center

	Pooling and Upsampling
		Max Pooling --> getting the most salient information.

		No learnable parameters

		Upsampling
			Increases the size of the input

			Nearest Neighbours Upsampling
				Same pixels for all in the kernel

			Linear Interpolation

			Bi-Linear Interpolation

	Transposed Convolutions
		Input * Filter -> Output
		 2x2      2x2       3x3

		Checkerboard problem

		They have learnable parameters
			Checkerboard pattern problem

	Upsampling infers pixels using a predefined method, while transposed convolution learns a filter.

	https://distill.pub/2016/deconv-checkerboard/

ASSIGNMENT
	# Deep Convolutional GAN (DCGAN)

	Here are the main features of DCGAN (don't worry about memorizing these, you will be guided through the implementation!):

		Use convolutions without any pooling layers
		Use batchnorm in both the generator and the discriminator
		Don't use fully connected hidden layers
		Use ReLU activation in the generator for all layers except for the output, which uses a Tanh activation.
		Use LeakyReLU activation in the discriminator for all layers except for the output, which does not use an activation

	<doubt where="1st cell">
		Why (image_tensor+1)/2
	</doubt>


	Transposed convolution, also known as fractionally strided convolution or deconvolution (although it's not a true deconvolution operation), is a technique used in neural networks for upsampling or generating higher-resolution outputs from lower-resolution inputs. The term "transpose" refers to the operation being the inverse of a regular convolution.

		Here are a few reasons why transposed convolutions are used:

		1. Upsampling: Transposed convolutions can effectively increase the spatial dimensions of a feature map. By applying transposed convolutions to a low-resolution feature map, you can upsample it to a higher resolution. This is useful in tasks such as image super-resolution, where you want to generate a higher-resolution image given a low-resolution input.

		2. Generating feature maps: Transposed convolutions can also be used to generate feature maps. In some cases, they can learn to produce patterns or structures that resemble the original inputs. For example, in image generation tasks, transposed convolutions are often used in the decoder part of a generative model to generate realistic images from a low-dimensional latent space.

		3. Learning spatial relationships: Transposed convolutions help to learn spatial relationships between features. They allow the network to learn to propagate information across the spatial dimensions, which can be useful in tasks such as semantic segmentation, where you want to predict the class label for each pixel in an image.

		4. Model symmetry: In certain network architectures, such as autoencoders or fully convolutional networks, transposed convolutions can be used to maintain symmetry between the encoder and decoder parts of the network. This symmetry can help in achieving better performance and stability during training.

		It's important to note that the term "deconvolution" in the context of neural networks is a misnomer. Transposed convolutions are not true deconvolutions in the mathematical sense but rather an operation that achieves the opposite effect of a regular convolution. The term "transposed" is used to denote the inverse operation rather than its mathematical meaning.

	Inter-epoch observation
		When discriminator's loss increases, generator's loss decreases and vice-versa.
			Obviously.


<---------- Week 3 ---------->

Advanced techniques to reduce instances of GAN failure due to imbalances between the generator and discriminator! 

Implement a WGAN to mitigate unstable training and mode collapse using W-Loss and Lipschitz Continuity enforcement.

Mode Collapse
	Any peak on the density function is a mode.
	"Modes are peaks in the distribition of features"

	If you get 2 features for each digit,
		The after plotting
		We see that there are 10 modes corresponding to each digit

	Very Interesting Problem:-
		If the discriminator is weak in identifying 1 and 7,
			then the Generator will continiuously keep generating 1 and 7 to fool the Discriminator

		And this will cause no learning
			Leading to bad art

		Our aim is not to fool the discriminator

	Mode Collapse happens when the generator gets stuck in one mode.

	Mode collapse occurs when the generator gets stuck generating one mode. 
	The discriminator will eventually learn to differentiate the generator's fakes when this happens and outskill it, ending the model's learning.

	Also this can happen when one mode can easily fool the discriminator compared to other modes

Problem with BCE Loss
	GANs with this are prone to Mode Collapse, and other problems.

	Why BCE Loss causes vanishing gradients problem?

	MiniMax game played by Generator and discriminator
		Generator
			Wants to "maximize the cost"
			By making fake look like real

		Discriminator
			Wants to "minimize the cost"
			By classifying fake properly
				For fakes--> (1-y) part

	Objective in GANs
		After that, this then becomes 
			To make real and generated data distributions of features very similar.

		Generator
			Trying to make the distributions look alike
			Difficult to train

		Discriminator
			Trying to pull it apart
			Easier to train than generator

	"Vanishing Gradients Problem"
		When the discriminator improves too much, the function approximated by BCE Loss will contain flat regions

		Flat regions on the cost function = Vanishing Gradients
			Which means the 'generator' does not know how to improve


		The discriminator does not output useful gradients (feedback) for the generator when the real/fake distributions are far apart.

Earth Mover's Distance
	Solves the vanishing gradient problem of BCE Loss

	"Effort" to make the generated distribution equal to the real distribution
		Depends on the distance and amount moved

	This measure is not bounded by 0 and 1

	It is from 0 to infinity
		Like normal distance measures

	Earth Mover's Distance is a function of amount and distance

	Doesn't have flat regions when the distributions are very different

	Approximating EMD solves the problems associated with BCE

Wasserstein Loss(W-Loss)
	Approximates the EMD

	x_hat = g(z)
		where z is noise vector input given to the generator

	min{over g}(
		min{over c}(
			Expectation(c(x)) - Expectation(c(x_hat))
		)
	)

		This concept is so good

	W-value --> any real value

	Discriminator --> bounded between 0 and 1

	In context of this, 
		we call it Critic --> any real value

W-Loss vs BCE Loss
	{Discriminator outputs between 0 and 1} whereas {Critic outputs any number}

	{Cost Function} vs {Cost Function}

	W-Loss helps with mode collapse and vanishing gradients problem

		Because the generator always get a useful feedback

Condition on Wasserstein Critic
	Continuity Condition on the critic's neural network

	The critic function
		c(x)
	must be 1-Lipshitz Continuous

		The norm of the gradient should be atmost 1 for every point

	Put a "X" mark and it forms 4 regions
		Now the curve has to be in left and right region

		It must not be in left region

	This makes
		W-Loss is valid

	Needed for training stable neural networks with W-Loss

	This condition ensures that W-Loss is validly approximating Earth Mover's Distance

	Notation:-
		<expression>
			<norm p=2>
				<gradient type=nebla>
					<function name=f>
						x
					<function>
				</gradient>
			</norm>

			<less_than></less_than>
			1
		</expression>

1-Lipschitz Continuity Enforcement
	Weight clipping and gradient penalty

	Advantages of gradient penalty


	Weight Clipping
		Weight clipping forces the weights of the critic to a fixed interval

		Methodology:-
			Gradient descent to update weights --> Clip the critic's weights

		The weights above or below are set to the highest limit and the lowest limit respectively.

		Limits the learning ability of the critic

		Lot of hyperparameter tuning involved

	Gradient Penalty
		MinMax equation + lambda * reg

		lambda * reg --> Regularization of the critic's gradient

		The lambda * reg penalizes the critic when its gradient norm is higher than 1

		Random interpolation
			Give epsilon weight to real 
				and 
			1 - epsilon weight to the generated image

			to obtain the "random interpolation"

		And this random interpolation, we want the critic's gradient to be less than 1

		<expression>
			<square>
				<norm p=2>
					<gradient type=nebla>
						<function name=f>
							x
						<function>
					</gradient>
				</norm>
				<minus></minus>
				1
			</square>
		</expression>

		Works much better than "weight clipping"

		Loss function is still continuous and differentiable

	Weight clipping is bit enforcing

	Gradient penalty tends to work better

	https://lilianweng.github.io/posts/2017-08-20-gan/

<---------- Week 4 ---------->

	Till now, no control over what the GANs generate
		"Unconditional Generation"
		Conditional Generation

		Controllable Generation

Conditional GAN
	A random sample from the class we specify

	Needs the data to be labelled

Conditional Generation: Inputs
	How to pass the class information to Generator and Discriminator?
		Discriminator --> needs one-hot vector

		Generator Input
			Noise vector and Class vector(one-hot) appended

		Discriminator Input
			The one-hot vector is converted to that many channels and append to the image's original 3 channels


	For the discriminator, the class information is appended as a channel or some other method, whereas for the generator, the class is encoded by appending a one-hot vector to the noise to form a long vector input.

Controllable Generation
	After trained

	Controlling specific features
		For example:- Controlling the age

	In the noise vector,
		if the first input represents the hair color
			then tweaking it would result in different hair color in output

	Controllable vs Conditional
		Controllable is sometimes a superset of Conditional

		Controllable --> features that we want
		Conditional  --> classes that we want

		Controllable --> dataset doesn't need to be labelled
		Conditional  --> Needs to be labelled

		Controllable --> Manipulate the z vector input
		Conditional  --> Append a class vector to the input

	Vector Algebra in Z-Space
		After finding a "5" in the Z-space and "6" in Z-space
			if we interpolate
				then we can find a morph of transition from 5 to 6 and it was displayed as a video

			5 morphed into a 6

		Just like in NLP	
			If u find that using a particular vector you were able to change the hair color from blue to red

			Then applying it to a different image also gives the same effect

		To control output features, you need to find directions in the Z-space

		To modify your output, you move around in the Z-space.

	Challenges with Controllable Generation
		Output feature correlation
		Z-space entanglement

		If features are super correlated, then its difficult to modify that one feature which is entangled with othe features

		Z-Space entanglement
			When Z-Space is entangled, movement in the different directions has an effect on multiple features simultaneously in the output
				even if those features aren't necessarily correlated (WHAAAAAAAATTTT)


		These kind of things happen when there is not enough features in Z-space
			relative to number of features that we want to control in output

		Correct! When the Z-space is entangled, movement in the different directions has an effect on multiple features simultaneously in the output. This makes it difficult to control a single feature without modifying others.

	Classifier Gradients
		Use classifiers to find directions in the Z-space

		Noise vector --> Generator --> Image --> Classifier --> *
			repeat

		Classifier tells the percentage of belief that the image produced has sunglasses
			and gives Direction to make the images look like they have sunglasses

			Modify just the noise vector until the feature emerges

			Generator model's parameters are not changed
				All of this happens after training

			Lazy finding of the "direction of feature"
				Lazy as in we are using a pre-classifier which knows that which image has sunglass or not

		You can calculate the gradient of the z vectors along certain features through the classifier to find the direction to move the z vectors.

	Disentanglement
		Disentangled Z-Space
			If you want to control 2 features (hair color and length), you should not make the Z-space dim 2
				Because u r then not giving space to learn

			It may also be possible that
				The Noise vectors are Disentangled with respect to just thse two different features and the rest of the values dont mean anything

				This is because we want the size of the Z-vector to be larger than the number of features that we want to control.

				This makes the model easier to learm, becuase it lets the other values to take on different things over time during training. 

				The extra ones are used to adpat the model during training.

		Latent Factors of Variation
			The information from the noise vectors is not seen directly on the output, but they do determine how that output looks. 



		Noise vector --> sometimes referred as "latents"

		Encourage Disentanglement: Loss Function method
			Add Regularization term

			Unsupervised way
				Doesnt need labels

			The above one is supervised

	